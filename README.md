# ğŸ¤– Ollama TypeScript Client

A fun little TypeScript client that chats with your local Ollama AI models! ğŸš€

## âœ¨ What This Does

This simple client demonstrates:
- ğŸ“¡ Making HTTP requests to Ollama's REST API
- ğŸ¯ Type-safe interfaces for requests and responses
- ğŸ¤” Asking AI questions and getting smart answers
- ğŸ“Š Inspecting the full response (including context tokens)

## ğŸ› ï¸ Quick Start

### 1. Make Sure Ollama is Running ğŸƒâ€â™‚ï¸
```bash
# From the parent directory
cd ../ollama-docker
docker compose up -d

# Pull a model if you haven't already
docker exec -it ollama ollama pull llama3.2:1b
```

### 2. Install Dependencies ğŸ“¦
```bash
npm install
```

### 3. Run the Magic! âœ¨
```bash
npm run dev
```

## ğŸª What You'll See

The client will ask the AI: *"Explain machine learning in one sentence"* and show you:

- ğŸ’¬ **The AI's response** - A smart, concise explanation
- ğŸ” **Full response object** - Including metadata and context tokens
- âš¡ **Performance stats** - How long it took to generate

## ğŸ”§ Available Scripts

- `npm run dev` - Run with hot reload using tsx
- `npm run build` - Compile TypeScript to JavaScript
- `npm start` - Run the compiled version

## ğŸ¨ Customize It!

Want to try different things? Edit `src/index.ts`:

```typescript
const payload: OllamaRequest = {
    model: "llama3.2:1b",           // Try different models!
    prompt: "Your question here",    // Ask anything!
    stream: false                    // Set to true for streaming
};
```

## ğŸš€ Fun Experiments

- Try different models: `llama3.2:3b`, `codellama:7b`
- Ask coding questions, creative writing prompts, or trivia
- Enable streaming with `stream: true`
- Use the context array for conversation continuity

## ğŸ¤“ Tech Stack

- **TypeScript** - For type safety and better DX
- **tsx** - For fast development with hot reload
- **Native fetch** - No extra HTTP libraries needed!

---

Happy AI chatting! ğŸ‰ Ask your local models anything! ğŸ’­
